{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gabrielebenanti/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gabrielebenanti/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from itertools import groupby\n",
    "import spacy\n",
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_words = ['alcool', 'cool', 'preesame', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_disguised_bad_words(text):\n",
    "    text = \" \" + text + \" \"\n",
    "    print(text)\n",
    "    text = re.sub(r' c[glo.x*@%#$^]+i ', ' coglioni ', text)\n",
    "    text = re.sub(r' c[glo.x*@%#$^]+e ', ' coglione ', text)\n",
    "    text = re.sub(r' c[az.x*@%#$^]+o ', ' cazzo ', text) \n",
    "    text = re.sub(r' c[az.x*@%#$^]+ro ', ' cazzaro ', text) \n",
    "    text = re.sub(r' c[az.x*@%#$^]+i ', ' cazzi ', text) \n",
    "    text = re.sub(r' m[erd.x*@%#$^]+a ', ' merda ', text) \n",
    "    text = re.sub(r' m[erd.x*@%#$^]+@ ', ' merda ', text)\n",
    "    text = re.sub(r' m[erd.x*@%#$^]+e ', ' merde ', text) \n",
    "    text = re.sub(r' c[.x*@%#$^]+lo ', ' culo ', text) \n",
    "    text = re.sub(r' p[tu.x*@%#$^]+a ', ' puttana ', text)\n",
    "    text = re.sub(r' p[tu.x*@%#$^]+e ', ' puttane ', text)\n",
    "    text = re.sub(r' t[.x*@%#$^]+a ', ' troia ', text)\n",
    "    text = re.sub(r' t[.x*@%#$^]+e ', ' troie ', text)\n",
    "    text = re.sub(r' s[.x*@%#$^]+o ', ' stronzo ', text)\n",
    "    text = re.sub(r' s[.x*@%#$^]+i ', ' stronzi ', text)\n",
    "    return text\n",
    "\n",
    "def deleteDuplicate (riga):\n",
    "    parole = riga.split()\n",
    "    stringa = \"\"\n",
    "    for a in parole:\n",
    "        parola = a\n",
    "        a = [list(g) for k, g in groupby(a)]    \n",
    "        vocali = ['a','e','i','o','u','y']\n",
    "        \n",
    "        for idx,_ in enumerate(a):\n",
    "            if idx == 0:\n",
    "                stringa += a[idx][0] \n",
    "            elif idx == len(a)-1:\n",
    "                stringa += a[idx][0]\n",
    "            elif a[idx][0] in vocali and (parola not in correct_words):\n",
    "                stringa += a[idx][0]\n",
    "            elif len(a[idx]) == 1:\n",
    "                stringa += a[idx][0]\n",
    "            elif len (a[idx]) >= 2:\n",
    "                stringa += a[idx][0]\n",
    "                stringa += a[idx][1]\n",
    "        stringa =  stringa + \" \"     \n",
    "    return(stringa)\n",
    "\n",
    "def replace_verbs_with_root(sentence):\n",
    "     # Dictionary to handle special cases manually\n",
    "    special_cases = {\n",
    "        \"c'Ã¨\": \"essere\",\n",
    "        \"c'erano\": \"essere\",\n",
    "        \"c'era\": \"essere\",\n",
    "        \"ci sono\": \"essere\"\n",
    "    }\n",
    "\n",
    "    # Replace special cases in the sentence\n",
    "    for word, root in special_cases.items():\n",
    "        sentence = sentence.replace(word, root)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # Remove newlines and carriage returns\n",
    "    tweet = tweet.replace('\\n', ' ').replace('\\r', ' ').lower()\n",
    "\n",
    "    # Remove incorrect bad word \n",
    "    tweet = clean_disguised_bad_words(tweet)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    tweet = deleteDuplicate(tweet)\n",
    "\n",
    "    # Substitute root verbs\n",
    "    tweet = replace_verbs_with_root(tweet)\n",
    "\n",
    "    # Substitute URLs with <URL>\n",
    "    tweet = re.sub(r'\\[URL\\]', '<URL>', tweet)\n",
    "\n",
    "    # Substitute mentions with <MENTION>\n",
    "    tweet = re.sub(r'@\\w+', '<MENTION>', tweet)\n",
    "\n",
    "    # Substitute hashtags with <HASHTAG>\n",
    "    tweet = re.sub(r'#\\w+', '<HASHTAG>', tweet)\n",
    "\n",
    "    # Replace emojis with text\n",
    "    tweet = emoji.demojize(tweet, language='it')\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "\n",
    "    # Remove stop words (Italian)\n",
    "    stop_words = set(stopwords.words('italian'))\n",
    "    filtered_tokens = \" \".join(word for word in tokens if word not in stop_words)\n",
    "\n",
    "    \"\"\" # Stemming using Italian stemmer\n",
    "    stemmer = SnowballStemmer('italian')\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens] \"\"\"\n",
    "\n",
    "    # Lemmatize using spaCy\n",
    "    doc = nlp(filtered_tokens)\n",
    "    lemmatize_tokens = [token.lemma_ if token.lemma_ != '-PRON-' else token.text for token in doc]    \n",
    "\n",
    "    # Remove special characters and numbers, preserve placeholders\n",
    "    processed_tweet = \" \".join(re.sub(r'[^a-zA-Z\\<\\>]', ' ', token) for token in lemmatize_tokens)\n",
    "\n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ma davvero ðŸ“º #salvinimerda49milioni  alcool c'Ã¨ che l'ha ciaaaooo c@lo ca**o m*rd@ una raccolta firme per #nocoprifuocodeimieicoglioni riuscirÃ  a far cambiare idea al #governodeipeggiori? povero illuso del cazzo \n",
      "Processed Tweet: davvero   televisore   < HASHTAG > alcool essere lo avere ciao culo cazzo merda raccoltare firma < HASHTAG > riuscire fare cambiare idea < HASHTAG >   povero illuso cazzo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example Italian tweet with placeholders\n",
    "tweet = \"Ma davvero ðŸ“º #SalviniMerda49Milioni  alcool c'Ã¨ che l'ha ciaaaooo c@lo ca**o m*rd@ una raccolta firme per #NoCoprifuocoDeiMieiCoglioni riuscirÃ  a far cambiare idea al #governodeipeggiori? Povero illuso del cazzo\"\n",
    "\n",
    "# Preprocess the tweet\n",
    "processed_tweet = preprocess_tweet(tweet)\n",
    "\n",
    "print(\"Processed Tweet:\", processed_tweet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
